---
title: "TinyModels as a Recommender System vs. N-Gram tokenization"
author: "Cliff Lee & Trang Do"
date: "11/14/2021"
output:
  html_document:
    df_print: paged
---
# Introduction 

This assignment initializes to apply NLP in building & classifying SPAM & HAM emails through training and testing datasets.

## TinyModels as a Recommender System

```{r 1, echo=FALSE, warning=FALSE, message=FALSE}
library(openintro)
library(DiceDesign)
library(tidymodels)
library(tidyverse)
library(workflows)
library(tune)
library(mlbench)
library(rsample)
library(recipes)
library(parsnip)
library(yardstick)
library(tm)
library(tidytext)
library(tm.plugin.mail)
```


```{r 2, echo=FALSE, warning=FALSE, message=FALSE}
github_repo <- "https://raw.githubusercontent.com/cliftonleesps/607_document_classification/master/"
sample_size <- 100
```

```{r 3}
# Define functions

# Helper function to paste full URLs when retrieving Github hosted files
full_github_url <- function(relative_path) {
  return (paste(github_repo, relative_path, sep="" ))
}

random_ham_file <- function() {
  # Select a different random message that's not part of the training set
  random_ham <- sample(unlist(ham_files), 1, replace = FALSE)
  
  # Repeat until we find a random spam file that is not in the training set
  while (any(str_detect(all_samples, random_ham))) {
    random_ham <- sample(unlist(ham_files), 1, replace = FALSE)
  }
  return (random_ham)
}


random_spam_file <- function() {
  # Select a different random message that's not part of the training set
  random_spam <- sample(unlist(spam_files), 1, replace = FALSE)
  
  # Repeat until we find a random spam file that is not in the training set
  while (any(str_detect(all_samples, random_spam))) {
    random_spam <- sample(unlist(spam_files), 1, replace = FALSE)
  }
  return (random_spam)
}


# Calculate a metric on a email subject. Range: 0.0 to 1.0
get_spam_subject_metric <- function(filename) { 
  spam_message <- read_file(full_github_url(filename))
  
  subject <- str_to_lower( str_match(spam_message, "\nSubject: (.*?)\n"))
  spam_subject_tibble <- tibble(
    text = subject[2]
  )
  spam_subject_words_tmp <- spam_subject_tibble %>% 
    unnest_tokens(word, text)
  spam_subject_word_count <- nrow(spam_subject_words_tmp)
  
  spam_subject_words_tmp <- inner_join(spam_subject_words_tmp, spam_subject_words_dictionary,by="word")
  spam_subject_metric <- nrow(spam_subject_words_tmp) / spam_subject_word_count
  return (spam_subject_metric)
}

```


```{r 4}

# Calculate a metric on a message body compared to other spam messages. Range: 0.0 to 1.0
get_spam_body_metric <- function(filename) { 
  # retrieve the file from Github and store locally in a hidden file
  hidden_filename <- '.tmp.txt'
  github_file <- read_file(full_github_url(filename))
  write_file(github_file, hidden_filename)
  
  email_message <- VCorpus(MBoxSource(hidden_filename), readerControl = list(reader = readMail))
  
  if (length(email_message) == 0) {
    return (0)
  }
  
  message_body <- email_message[[1]]$content
  message_body_tibble <- tibble(
    text = message_body
  )
  message_body
  message_body_tibble <- message_body_tibble %>% 
    unnest_tokens(word, text, format="html")
  message_body_count <- nrow(message_body_tibble)
  spam_body_tibble <- inner_join(message_body_tibble, spam_words_dictionary,by="word")
  spam_body_metric <- nrow(spam_body_tibble) / message_body_count
  return (spam_body_metric)
}

# Calculate a metric on a message body compared to other ham messages. Range: 0.0 to 1.0
get_ham_body_metric <- function(filename) {
  # retrieve the file from Github and store locally in a hidden file
  hidden_filename <- '.tmp.txt'
  github_file <- read_file(full_github_url(filename))
  write_file(github_file, hidden_filename)

  email_message <- VCorpus(MBoxSource(hidden_filename), readerControl = list(reader = readMail))
  
  if (length(email_message) == 0) {
    return (0)
  }
  
  message_body <- email_message[[1]]$content
  message_body_tibble <- tibble(
    text = message_body
  )
  message_body
  message_body_tibble <- message_body_tibble %>% 
    unnest_tokens(word, text, format="html")
  message_body_count <- nrow(message_body_tibble)

  ham_body_tibble <- inner_join(message_body_tibble, ham_words_dictionary,by="word")
  ham_body_metric <- nrow(ham_body_tibble) / message_body_count
  return (ham_body_metric)
}

```


```{r 5}
# Read the various dictionaries
spam_subject_words_dictionary <- read_csv(
                                          full_github_url("spam_subject_words.csv"),
                                          show_col_types = FALSE)
spam_words_dictionary <- read_csv(
                                  full_github_url("spam_words.csv"),
                                  show_col_types = FALSE)


ham_words_dictionary <- read_csv(
                                full_github_url("ham_words.csv"),
                                show_col_types = FALSE
                                )
ham_words_dictionary

full_github_url("spam_files.csv")

# Retrieve the lists of spam and ham files
spam_files <- read_csv(
                      full_github_url("spam_files.csv"), 
                      show_col_types = FALSE)

ham_files <- read_csv(
                      full_github_url("ham_files.csv"),
                      show_col_types = FALSE)


# Randomly select both types of files
all_samples <- sample(unlist(ham_files), sample_size, replace = FALSE)
all_samples <- append(all_samples,
                     sample(unlist(spam_files), sample_size, replace = FALSE) )

# now load the messages from Github
email_messages <- tibble()
count <- 0
for (fn in all_samples) {
  #print (paste(fn, count))

    # Determine the type of message
  if (str_detect(fn, "ham/")) {
    type <- "ham"
  } else {
    type <- "spam"
  }

  new_observation <- tibble(
                            name = fn,
                            type = type,
                            spam_subject_metric = get_spam_subject_metric(fn),
                            spam_body_metric = get_spam_body_metric(fn),
                            ham_body_metric = get_ham_body_metric(fn)
                            )
  email_messages <- bind_rows(email_messages, new_observation)
  count <- count + 1
}
email_messages
```

### The results from TinyModels 

The accuracy of classifying SPAM/HAM achieves 95%

## N-Gram

### First Attempt

All HAM & SPAM files are loaded and random choose to run

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(tidytext)
library(tidyverse)
library(gridExtra)
library(wordcloud)
library(tidyr)
library(ggplot2)
library(dplyr)
library(ggthemes)
library(DBI)

spam_path <- "/Users/admin/Desktop/DATA607/project4/spam/"
ham_path <- "/Users/admin/Desktop/DATA607/project4/easy_ham/"

spam_filelist <-list.files(spam_path)
spam_filelist <- as.data.frame(spam_filelist)
spam_filelist<-rename(spam_filelist,file=spam_filelist)
spam_filelist["type"]<-"spam"
ham_filelist <-list.files(ham_path)
ham_filelist <- as.data.frame(ham_filelist)
ham_filelist<-rename(ham_filelist,file=ham_filelist)
ham_filelist["type"]<-"ham"

filelist <- rbind(ham_filelist,spam_filelist)

set.seed(80)
sample<-sample_n(filelist,100)
train <- sample[1:80,]
test <- sample[81:100,]
data.raw <- data.frame(x='',y='',type='')

for (i in 1:80){
  if (train$type[i]=='spam'){
    path <- "/Users/admin/Desktop/DATA607/project4/spam/"
  }else{
    path <- "/Users/admin/Desktop/DATA607/project4/easy_ham/"
  }
  path<- paste(path,train$file[i],sep="")
  ham_spam<- read_lines(path,skip=0,n_max = -1L)
  temp<-as.data.frame(merge(train$file[i],ham_spam))
  temp["type"] <- train$type[i]
  data.raw <- rbind.data.frame(data.raw,temp)
}

```

### Naive Bayes Method

P(N) : Probability of HAM over total emails

P(S) : Probability of SPAM over total emails

P(W|S): Probability of words on SPAM

P(W|N): Probability of words on HAM

The dataset hasn't been validate whether it's the best fit yet. When a new email comes, it will be tokernized based on ngram. Each word will calculate P(W|H) & P(W|S) from sample dataset to compare and classify as SPAM or HAM

### Reading Data with ngram tokenization.

```{r reading-data,echo=FALSE, message=FALSE, warning=FALSE}
df <- as.data.frame(data.raw %>% unnest_tokens(ngram, y, token = "ngrams", n = 1))
#test <- spam.raw %>% unnest_tokens(word,y, format = "html")

df <- df %>% filter(!is.na(ngram)) %>%
  anti_join(stop_words, by=c("ngram"="word"))
#--------------
dict <- unique(df %>% select(ngram)) 
temp <- unique(df %>% filter(type=='ham') %>%
      select(ngram)%>% 
      group_by(ngram)%>% mutate(h.word.count =n()))
dict<- full_join(dict,temp,by="ngram")

temp <- unique(df %>% filter(type=='spam') %>%
      select(ngram)%>% 
      group_by(ngram)%>% mutate(s.word.count =n()))
dict<- full_join(dict,temp,by="ngram")

dict["h.total.word.count"]<-count(df %>% filter(type=='ham')) 
dict["s.total.word.count"]<-count(df %>% filter(type=='spam')) 

dict[is.na(dict)] = 0

dict<- dict %>% mutate(s.word.count=s.word.count+1,h.word.count=s.word.count+1,
                       p.word.s=s.word.count/s.total.word.count,
                       p.word.h=h.word.count/h.total.word.count)

temp<- nrow(unique(df %>%filter(type=='ham') %>% 
                select(x,type) %>%
                group_by(x)))
       
dict["h.docs.with.word"] <- temp

temp<- nrow(unique(df %>%filter(type=='spam') %>% 
                select(x,type) %>%
                group_by(x)))
       
dict["s.docs.with.word"] <- temp

dict[is.na(dict)] = 0
dict <- dict %>% mutate(p.h = h.docs.with.word/(s.docs.with.word+h.docs.with.word),
                        p.s = s.docs.with.word/(s.docs.with.word+h.docs.with.word))
```

### Plots words from HAM

```{r plot_Ham,echo=FALSE, message=FALSE, warning=FALSE}
ham_wordcloud <- top_n(dict %>% select(ngram,h.word.count) %>%
  arrange(desc(ngram,h.word.count)),n=300)

wordcloud(ham_wordcloud$ngram,freq = ham_wordcloud$h.word.count,scale=c(3.5,0.25),
          colors=brewer.pal(8,"Dark2"))

ggplot(ham_wordcloud, aes(h.word.count),horizontal = TRUE) + 
  geom_histogram()

ggplot(top_n(ham_wordcloud,50), aes(x=reorder(ngram,h.word.count),y =h.word.count)) + 
  geom_bar(stat='identity',fill="olivedrab")+
  coord_flip()+
  ylab("Count")+
  xlab("ngram")+
  theme_tufte()+
  ggtitle("HAM")  
```

### Plots words from SPAM

```{r plot_spam, echo=FALSE, message=FALSE, warning=FALSE}
spam_wordcloud <- top_n(dict %>% select(ngram,s.word.count) %>%
  arrange(desc(ngram,s.word.count)),n=300)

wordcloud(spam_wordcloud$ngram,freq = spam_wordcloud$s.word.count,scale=c(3.5,0.25),
          colors=brewer.pal(8,"Dark2"))

ggplot(spam_wordcloud, aes(s.word.count),horizontal = TRUE) + 
  geom_histogram()

ggplot(top_n(spam_wordcloud,50), aes(x=reorder(ngram,s.word.count),y =s.word.count)) + 
  geom_bar(stat='identity',fill="olivedrab")+
  coord_flip()+
  ylab("Count")+
  xlab("ngram")+
  theme_tufte()+
  ggtitle("SPAM")  
```

### Using library(tm.plugin.mail) 

hamd.rmd & spam.rmd use VCorpus(MBoxSource(fn), readerControl = list(reader = readMail)) to separate email body, header, subject.... . Emails content are saved in PostgreSQl to retrieve.

Retrieve SPAM from PostgreSQL to an
```{r postgresql, echo=FALSE, message=FALSE, warning=FALSE}
db <- 'project4'  #provide the name of your db
host_db <- 'localhost'
db_port <- '5432'
db_user <- 'postgres' 
db_password <- 'data607'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password) 

spam.content<- dbGetQuery(con, "select * from emailcontent where COALESCE(content, '') != '' and SUBSTRING ( file ,1, 4 )='spam'") 

spam.df <- as.data.frame(spam.content %>% unnest_tokens(ngram, content, token = "ngrams", n = 1))
#View(test)
test <- spam.df  %>% select(ngram) %>% 
  anti_join(stop_words, by=c("ngram"="word")) 

test <- unique(test %>% group_by(ngram) %>% 
                mutate(count = n()))

test<- test %>% arrange(desc(count))

ggplot(top_n(ungroup(test),50), aes(x=reorder(ngram,count),y =count)) + 
  geom_bar(stat='identity',fill="olivedrab")+
  coord_flip()+
  theme_tufte()+
  ylab("Count")+
  xlab("ngram")

temp <- test %>% mutate(ratio=count/sum(test$count))

ggplot(temp, aes(count),horizontal = TRUE) + 
  geom_histogram(aes(x=count,y=..density..),bins=30) +
  geom_density(color="brown")
  
ggplot(temp, aes(sample=count))+
  stat_qq()+
  stat_qq_line()+
  facet_grid()

ggplot(temp, aes(sample=ratio))+
  stat_qq()+
  stat_qq_line()+
  facet_grid()

p1<-qplot(x=count,data=temp)
p2<-qplot(x=sqrt(count),data=temp)
p3<-qplot(x=log10(count),data=temp)

grid.arrange(p1,p2,p3, nrow=1, ncol=3)

normalized.data <- temp %>% mutate(sqrt.freq=sqrt(count),log10.freq=(count))
ggplot(normalized.data , aes(sample=sqrt.freq))+
  stat_qq()+
  stat_qq_line()+
  facet_grid()

ggplot(normalized.data , aes(sample=log10.freq))+
  stat_qq()+
  stat_qq_line()+
  facet_grid()

p1<- ggplot(normalized.data , aes(sample=count))+
  stat_qq()+
  stat_qq_line()+
  facet_grid()

p2<-ggplot(normalized.data , aes(sample=sqrt.freq))+
  stat_qq()+
  stat_qq_line()+
  facet_grid()

p3<- ggplot(normalized.data , aes(sample=log10.freq))+
  stat_qq()+
  stat_qq_line()+
  facet_grid()

grid.arrange(p1,p2,p3, nrow=1, ncol=3)

Q <- quantile(temp$count, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(temp$count)

up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range

eliminated<- subset(temp, temp$count > (Q[1] - 1.5*iqr) & temp$count  < (Q[2]+1.5*iqr))

outliers <- boxplot(temp$count, plot=FALSE)$out

remove.outliers<-temp
remove.outliers <- remove.outliers[-which(remove.outliers$count %in% outliers),]

x_wordcloud <- top_n(remove.outliers %>% select(ngram,count) %>%
  arrange(desc(ngram,count)),n=300)
wordcloud(x_wordcloud$ngram,freq = x_wordcloud$count,scale=c(3.5,0.25),
          colors=brewer.pal(8,"Dark2"))

par(mfrow=c(1,2))
boxplot(temp$count,main="Before removing outliers")
boxplot(remove.outliers$count,main="After removing outliers")
par("mfrow")


# ----------------

```

### Observation from first attempt
N-Gram & stop_words are not sufficient tools for classifying SPAM & HAM. The structure of an email is different from a book, article, or essay paragraph. An email has a header, sender & recipient(s) information, IP addresses besides the content and footer of the email. An email is maybe formatted in plain text or HTML. After the first attempt, words with high frequency are similar in HAM and SPAM. They are related to email format. Outliers are removed based on the boxplot. However, quickly removing outliers could lead to an accurate corpus for classifying HAM/SPAM.